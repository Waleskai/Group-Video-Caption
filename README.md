# Group-Video-Caption

This repository for the paper *Exploring Group Video Captioning with Efficient Relational Approximation.*

### Datasets

We construct two datasets with pairs of a video group and its shared content description on top of two existing dense video captioning datasets: ActivityNet Captions dataset, and YouCook2.

Each data sample consists of a group of target videos with their shared content description and a larger group of reference videos. The reference videos need to be relevant to target videos while containing a larger variety of visual content and thus providing context for describing target videos. The  description should be simultaneously specific to the target group and conditioned on the reference group.

**Now, you can download [ActivityNet Captions](https://drive.google.com/drive/u/1/folders/1RwXealf9B1A89YoMUNU5FtD_Xsr9X5T3), and [YouCook2](https://drive.google.com/drive/u/1/folders/1RwXealf9B1A89YoMUNU5FtD_Xsr9X5T3).**
